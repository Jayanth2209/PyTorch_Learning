{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d600d4a49ef54742afdda94fc2b25f0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_504fc5e56fa74bbf805426370760353e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2e79dcdeb6464bc0a6e8e23ccdbda3f3",
              "IPY_MODEL_222ad3e1ff984ecebf24685a9fed86dc"
            ]
          }
        },
        "504fc5e56fa74bbf805426370760353e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e79dcdeb6464bc0a6e8e23ccdbda3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0082fb8644ff4704b1473bdb171d95b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9912422,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9912422,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6489a9c8dcc4a988fdcaeabf86338cd"
          }
        },
        "222ad3e1ff984ecebf24685a9fed86dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1bfd7aee40ca4f82992808a8377f61d9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9913344/? [00:09&lt;00:00, 1026694.08it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab598fae1e2747ce8c3c0343abdc1a78"
          }
        },
        "0082fb8644ff4704b1473bdb171d95b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6489a9c8dcc4a988fdcaeabf86338cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1bfd7aee40ca4f82992808a8377f61d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab598fae1e2747ce8c3c0343abdc1a78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd28c854a06c4e5b94c5ddfda24af516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_91d3b32f27f341eabede909af04ce1ff",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_68d90c5053024cc498a2287f3a5e59a5",
              "IPY_MODEL_81a2b3484f9e4da6aef3f7eeacd27ddb"
            ]
          }
        },
        "91d3b32f27f341eabede909af04ce1ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68d90c5053024cc498a2287f3a5e59a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9965629dbe334ed5abeda04207aa7e62",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e393d9229de04f1787c98213ac52f551"
          }
        },
        "81a2b3484f9e4da6aef3f7eeacd27ddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_abb90e350c0342009b78b9fb6bf84109",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 29696/? [00:02&lt;00:00, 12183.55it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5f7d37c91814feab47e3ef3922e9623"
          }
        },
        "9965629dbe334ed5abeda04207aa7e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e393d9229de04f1787c98213ac52f551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "abb90e350c0342009b78b9fb6bf84109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5f7d37c91814feab47e3ef3922e9623": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cec7ad2c698f413f8be9c399df075b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8ccd9df68f5848a2984165d5c7434084",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3d3ce92445fa418c89a0c306131dee69",
              "IPY_MODEL_4c826901f1034977a73cb8ff679463dd"
            ]
          }
        },
        "8ccd9df68f5848a2984165d5c7434084": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d3ce92445fa418c89a0c306131dee69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ab161a1029594f09814a8d0eb192746c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1648877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1648877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7a0e3e4e98744e7b6bba462623bf472"
          }
        },
        "4c826901f1034977a73cb8ff679463dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_210690d78af849109d72c22a3dcbda45",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1649664/? [00:03&lt;00:00, 440358.61it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83ec6f7060d0447db3480b96daeecd51"
          }
        },
        "ab161a1029594f09814a8d0eb192746c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7a0e3e4e98744e7b6bba462623bf472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "210690d78af849109d72c22a3dcbda45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83ec6f7060d0447db3480b96daeecd51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e21c23a60a1b4609a6038cc04588e536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d16f7e3971a846a19012fce177ae2bf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_41c536b2ab0e4f5c85af2cb801316c66",
              "IPY_MODEL_3d73d34befad48d4b7eae694235212eb"
            ]
          }
        },
        "d16f7e3971a846a19012fce177ae2bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41c536b2ab0e4f5c85af2cb801316c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_38ead8b9f0cf4609a9f6ab8b028923e4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4542,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4542,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_321510c21e2b46a1a97871f73b187318"
          }
        },
        "3d73d34befad48d4b7eae694235212eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ee65eb7ddee7494ea2614ff16088122a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5120/? [00:00&lt;00:00, 18911.88it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36c66b3d125c47a099b6feaac08a1344"
          }
        },
        "38ead8b9f0cf4609a9f6ab8b028923e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "321510c21e2b46a1a97871f73b187318": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee65eb7ddee7494ea2614ff16088122a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36c66b3d125c47a099b6feaac08a1344": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQTowZ7L3r_r"
      },
      "source": [
        "# Logistic Regression using MNIST\n",
        "\n",
        "We'll use the famous [MNIST Handwritten Digits Database](http://yann.lecun.com/exdb/mnist/) for training. It consists of 28x28 grayscale images of handwritten digits (0 to 9) and labels for each image indicating which digit it represents.\n",
        "![mnist-sample](https://i.imgur.com/CAYnuo1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XaAmiRq4HHt"
      },
      "source": [
        "Import `torch` and `torchvision`. \n",
        "\n",
        "`torchvision` contains some utilities for working with image data. It also provides helper classes, to download and import popular datasets like MNIST automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvuY11la4VPY"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736,
          "referenced_widgets": [
            "d600d4a49ef54742afdda94fc2b25f0a",
            "504fc5e56fa74bbf805426370760353e",
            "2e79dcdeb6464bc0a6e8e23ccdbda3f3",
            "222ad3e1ff984ecebf24685a9fed86dc",
            "0082fb8644ff4704b1473bdb171d95b3",
            "b6489a9c8dcc4a988fdcaeabf86338cd",
            "1bfd7aee40ca4f82992808a8377f61d9",
            "ab598fae1e2747ce8c3c0343abdc1a78",
            "fd28c854a06c4e5b94c5ddfda24af516",
            "91d3b32f27f341eabede909af04ce1ff",
            "68d90c5053024cc498a2287f3a5e59a5",
            "81a2b3484f9e4da6aef3f7eeacd27ddb",
            "9965629dbe334ed5abeda04207aa7e62",
            "e393d9229de04f1787c98213ac52f551",
            "abb90e350c0342009b78b9fb6bf84109",
            "b5f7d37c91814feab47e3ef3922e9623",
            "cec7ad2c698f413f8be9c399df075b08",
            "8ccd9df68f5848a2984165d5c7434084",
            "3d3ce92445fa418c89a0c306131dee69",
            "4c826901f1034977a73cb8ff679463dd",
            "ab161a1029594f09814a8d0eb192746c",
            "f7a0e3e4e98744e7b6bba462623bf472",
            "210690d78af849109d72c22a3dcbda45",
            "83ec6f7060d0447db3480b96daeecd51",
            "e21c23a60a1b4609a6038cc04588e536",
            "d16f7e3971a846a19012fce177ae2bf1",
            "41c536b2ab0e4f5c85af2cb801316c66",
            "3d73d34befad48d4b7eae694235212eb",
            "38ead8b9f0cf4609a9f6ab8b028923e4",
            "321510c21e2b46a1a97871f73b187318",
            "ee65eb7ddee7494ea2614ff16088122a",
            "36c66b3d125c47a099b6feaac08a1344"
          ]
        },
        "id": "URTnkRo64Wcg",
        "outputId": "3e590ec9-06a6-457a-c57e-9b1bbb537309"
      },
      "source": [
        "# Download dataset for training\n",
        "dataset = MNIST(root='data/', download=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d600d4a49ef54742afdda94fc2b25f0a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd28c854a06c4e5b94c5ddfda24af516",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cec7ad2c698f413f8be9c399df075b08",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e21c23a60a1b4609a6038cc04588e536",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmXdpEB45LW"
      },
      "source": [
        "The above command downloads the data (the first time it is run) to the `data/` directory (Check Files) and creates a PyTorch `Dataset`. On subsequent executions, the download is skipped as the data is already downloaded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIEGJCK14glA",
        "outputId": "e54b26ab-2230-4299-fbfc-92a9d92b9274"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgEkItXF5TYE"
      },
      "source": [
        "The dataset has 60,000 images used to train the model. There is also an additional test set of 10,000 images used for testing and evaluating the model. We can create the test dataset using the `MNIST` class by passing `train=False` to the constructor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH9X7Re842Y6",
        "outputId": "5054cc19-1038-42f7-881d-7817d032ffd0"
      },
      "source": [
        "test_dataset = MNIST(root='data/', train=False)\n",
        "len(test_dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_T_9urr5klw"
      },
      "source": [
        "A sample in the dataset looks as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STIRsk5Y5gr8",
        "outputId": "cf04c074-6581-47d3-a392-b3da6d2a9e84"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=L size=28x28 at 0x7FB10F798650>, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5hv7qXS5sxM"
      },
      "source": [
        "It is an image of size 28x28. The image is an object of class `PIL.Image.Image`,which is a part of Python imaging library **Pillow**. We can use `matplotlib` to view the image in our Colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgnT7Wk25q3F"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1eV-YX26Kh4"
      },
      "source": [
        "`%matplotlib inline` indicates to Colab that we want to plot the graphs within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yxDe_pCZ6J4g",
        "outputId": "51a1fb39-f746-47a3-8e5f-543b3efc8f69"
      },
      "source": [
        "image, label = dataset[0]\n",
        "plt.imshow(image, cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXZWLF_e6b0g"
      },
      "source": [
        "PyTorch doesn't know how to work with images in their raw form. So, we need to convert these images to Tensors. We do this by specifying a transform while creating our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB4jwuNm6XSf"
      },
      "source": [
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BkrHbzG6udW"
      },
      "source": [
        "PyTorch datasets allow us to specify one or more transformation functions that are applied to the images as they are loaded. The `torchvision.transforms` module contains many such predefined functions. We'll use the `ToTensor` transform to convert images into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEE1CVwk6uEU"
      },
      "source": [
        "dataset = MNIST(root='data/', \n",
        "                train=True,\n",
        "                transform=transforms.ToTensor())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqsMRu3A7PJ-",
        "outputId": "f67806e4-369d-4144-ab09-a25a5a475998"
      },
      "source": [
        "# Example\n",
        "img_tensor, label = dataset[0]\n",
        "#print(img_tensor)\n",
        "print(img_tensor.shape, label) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28]) 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFejGuN-7iME"
      },
      "source": [
        "The image is now converted to a 1x28x28 tensor. The first index/dimension corresponds to colour channel. This is a grayscale image and hence has only one colour channel. An RGB image has 3 channels each for R, G and B.\n",
        "\n",
        "If we print the `img_tensor`, we can view the array representation. The values range from 0 to 1 - 0 corresponds to black and 1 to white.\n",
        "\n",
        "We can also print and show a specific part of an image as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "cy0cazi-7W1e",
        "outputId": "613d9ef9-d8b2-4ef4-d761-2aa2811f8cbb"
      },
      "source": [
        "print(f'Image Tensor: {img_tensor[0,10:15,10:15]}\\nImage Section:')\n",
        "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Tensor: tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
            "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
            "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
            "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
            "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\n",
            "Image Section:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb10f6eb190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8SChThc8tef"
      },
      "source": [
        "Now, for training our model, we need to split the dataset into 3 parts:\n",
        "* Training Set\n",
        "* Validation Set\n",
        "* Test Set\n",
        "\n",
        "**Validation set** is used to evaluate the model during training - for hyperparameter tuning, etc.\n",
        "\n",
        "For obtaining our validation set, let us randomly choose 20% of images from our training set of 60000 images i.e., we will be using 48000 images for training and 12000 images for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "_4Mgz4uK-Erm",
        "outputId": "684e7876-83c8-4927-9836-c6782a85c344"
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [48000, 12000])\n",
        "len(train_ds), len(val_ds)\n",
        "\n",
        "# Alternately\n",
        "'''import numpy as np\n",
        "\n",
        "def split_indices(n,val_pct):\n",
        "  # Determine the size of validation set\n",
        "  n_val = int(val_pct*n)\n",
        "  # Create a random permutation of 0 to n-1\n",
        "  indices = np.random.permutation(n)\n",
        "  # Pick first n_val indices for validation set\n",
        "  return indices[n_val:], indices[:n_val]\n",
        "\n",
        "train_indices, val_indices = split_indices(len(dataset),val_pct=0.2)'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import numpy as np\\n\\ndef split_indices(n,val_pct):\\n  # Determine the size of validation set\\n  n_val = int(val_pct*n)\\n  # Create a random permutation of 0 to n-1\\n  indices = np.random.permutation(n)\\n  # Pick first n_val indices for validation set\\n  return indices[n_val:], indices[:n_val]\\n\\ntrain_indices, val_indices = split_indices(len(dataset),val_pct=0.2)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9IVWLtY-Mmr"
      },
      "source": [
        "It's essential to choose a random sample for creating a validation set. Training data is often sorted by the target labels, i.e., images of 0s, followed by 1s, followed by 2s, etc. If we create a validation set using the last 20% of images, it would only consist of 8s and 9s. In contrast, the training set would contain no 8s or 9s. Such a training-validation would make it impossible to train a useful model.\n",
        "\n",
        "We can now create data loaders to help us load the data in batches. We'll use a batch size of 128."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAHy8tfO8odJ"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GvhgLgw-ZUf"
      },
      "source": [
        "We set `shuffle=True` for the training data loader to ensure that the batches generated in each epoch are different. This randomization helps generalize & speed up the training process. \n",
        "\n",
        "Since the validation data loader is used only for evaluating the model, there is no need to shuffle the images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OOXukvF-5SM"
      },
      "source": [
        "## Model\n",
        "* **Logistic regression** model is almost identical to a Linear regression model. It contains weights and bias matrices, and the output is obtained using matrix operations (`y = x @ w.t() + b`). \n",
        "\n",
        "* We can use `nn.Linear` to create the model instead of manually creating and initializing the matrices.\n",
        "\n",
        "* Since `nn.Linear` expects each training example to be a vector, each `1x28x28` image tensor is _flattened_ into a vector of size 784 `(28*28)` before being passed into the model. \n",
        "\n",
        "* The output for each image is a vector of size 10, with each element signifying the probability of a particular target label (i.e., 0 to 9). The predicted label for an image is simply the one with the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BULu0Mfs-Yhc"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_size = 28*28\n",
        "num_classes = 10\n",
        "\n",
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXCdCeoy_aLq",
        "outputId": "f8e70837-a34e-4bba-e5f8-dbadefa6c519"
      },
      "source": [
        "print(model.weight)\n",
        "print(model.weight.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0166, -0.0125,  0.0327,  ...,  0.0347, -0.0356, -0.0169],\n",
            "        [ 0.0033, -0.0295, -0.0227,  ...,  0.0177, -0.0047,  0.0190],\n",
            "        [ 0.0053,  0.0264, -0.0014,  ..., -0.0189, -0.0140,  0.0266],\n",
            "        ...,\n",
            "        [-0.0121,  0.0009, -0.0212,  ...,  0.0021,  0.0060, -0.0017],\n",
            "        [-0.0180,  0.0281,  0.0233,  ...,  0.0156, -0.0008,  0.0154],\n",
            "        [ 0.0107,  0.0295, -0.0073,  ...,  0.0199, -0.0172, -0.0213]],\n",
            "       requires_grad=True)\n",
            "torch.Size([10, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqU65UV7KN4m",
        "outputId": "04cd8f2e-0495-4493-fdf9-e7f1c219121d"
      },
      "source": [
        "print(model.bias)\n",
        "print(model.bias.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([-0.0112, -0.0247, -0.0135,  0.0073,  0.0102,  0.0284,  0.0002,  0.0063,\n",
            "         0.0084, -0.0351], requires_grad=True)\n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3mVjWf_KTBz"
      },
      "source": [
        "Therefore, there are a total of 784x10 + 10 = 7850 parameters in our model. Let's generate some outputs using the model. we'll take the first batch of 128 images and pass them to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "46tO5_E0KkhH",
        "outputId": "06a9ccc9-6a9e-44f0-d377-2b7cfbf5d7e3"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print(labels)\n",
        "    print(images.shape)\n",
        "    outputs = model(images)\n",
        "    print(outputs)\n",
        "    break"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 4, 4, 6, 5, 4, 2, 0, 6, 0, 5, 5, 0, 6, 1, 0, 1, 1, 2, 6, 6, 3, 5, 7,\n",
            "        6, 2, 1, 3, 9, 7, 5, 8, 3, 1, 7, 0, 6, 4, 6, 2, 3, 1, 3, 4, 8, 9, 2, 4,\n",
            "        9, 4, 8, 1, 7, 8, 0, 4, 6, 0, 6, 7, 1, 4, 4, 7, 4, 6, 9, 2, 9, 7, 8, 1,\n",
            "        1, 4, 4, 9, 7, 3, 6, 0, 2, 3, 8, 7, 1, 9, 4, 2, 0, 7, 3, 9, 4, 2, 6, 0,\n",
            "        4, 6, 6, 6, 9, 6, 2, 0, 0, 1, 1, 3, 4, 1, 2, 6, 1, 7, 4, 3, 4, 2, 0, 8,\n",
            "        2, 8, 3, 6, 1, 1, 2, 1])\n",
            "torch.Size([128, 1, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d0fe7d306f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LiConUGPQUv",
        "outputId": "87ecca57-cbd5-4d50-e256-5c1b9af66608"
      },
      "source": [
        "images.reshape(batch_size,input_size).shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128, 784])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIreaUvKNvwm"
      },
      "source": [
        "This leads to an error because our input data does not have the right shape. Our images have a shape of 1x28x28, but we need them to be vectors of size 784, i.e., we need to flatten them. We'll use `.reshape`, which will allow us to efficiently 'view' each image as a flat vector without really creating a copy of the underlying data. \n",
        "\n",
        "To include this additional functionality within our model, we need to define a custom model by extending the `nn.Module` class from PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJR_N6LJLbpb"
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(input_size,num_classes)\n",
        "\n",
        "  def forward(self,xb):\n",
        "    xb = xb.reshape(-1,input_size)\n",
        "    out = self.linear(xb)\n",
        "    return out\n",
        "\n",
        "model = MnistModel()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kw0vhZwQN0s"
      },
      "source": [
        "Inside the `__init__` constructor method, we instantiate the weights and biases using `nn.Linear`. And inside the `forward` method, which is invoked when we pass a batch of inputs to the model, we flatten the input tensor and pass it into `self.linear`.\n",
        "\n",
        "`xb.reshape(-1, input_size)` indicates to PyTorch that we want a *view* of the `xb` tensor with two dimensions. The length along the 2nd dimension is 28*28 (i.e., 784). One argument to `.reshape` can be set to `-1` (in this case, the first dimension) to let PyTorch figure it out automatically based on the shape of the original tensor.\n",
        "\n",
        "Note that the model no longer has `.weight` and `.bias` attributes (as they are now inside the `.linear` attribute), but it does have a `.parameters` method that returns a list containing the weights and bias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wixYnnq1QM7f",
        "outputId": "e528c3b9-b1ce-4ded-e391-b778e4b5a85b"
      },
      "source": [
        "print(model.linear)\n",
        "print('\\n')\n",
        "print(f'Weights: {model.linear.weight}, Size: {model.linear.weight.shape}\\n')\n",
        "print(f'Biases: {model.linear.bias}, Size: {model.linear.bias.shape}\\n')\n",
        "list(model.parameters())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=784, out_features=10, bias=True)\n",
            "\n",
            "\n",
            "Weights: Parameter containing:\n",
            "tensor([[ 0.0206,  0.0291, -0.0148,  ..., -0.0271,  0.0177, -0.0310],\n",
            "        [-0.0063, -0.0327, -0.0147,  ...,  0.0086,  0.0027,  0.0295],\n",
            "        [ 0.0009,  0.0186, -0.0167,  ..., -0.0152, -0.0267,  0.0039],\n",
            "        ...,\n",
            "        [ 0.0064, -0.0264, -0.0129,  ..., -0.0105,  0.0055,  0.0282],\n",
            "        [-0.0333, -0.0237, -0.0112,  ..., -0.0124, -0.0302,  0.0266],\n",
            "        [ 0.0052, -0.0047, -0.0289,  ...,  0.0075,  0.0053, -0.0345]],\n",
            "       requires_grad=True), Size: torch.Size([10, 784])\n",
            "\n",
            "Biases: Parameter containing:\n",
            "tensor([ 0.0172,  0.0068, -0.0045, -0.0299, -0.0233, -0.0015, -0.0045, -0.0080,\n",
            "        -0.0347, -0.0191], requires_grad=True), Size: torch.Size([10])\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.0206,  0.0291, -0.0148,  ..., -0.0271,  0.0177, -0.0310],\n",
              "         [-0.0063, -0.0327, -0.0147,  ...,  0.0086,  0.0027,  0.0295],\n",
              "         [ 0.0009,  0.0186, -0.0167,  ..., -0.0152, -0.0267,  0.0039],\n",
              "         ...,\n",
              "         [ 0.0064, -0.0264, -0.0129,  ..., -0.0105,  0.0055,  0.0282],\n",
              "         [-0.0333, -0.0237, -0.0112,  ..., -0.0124, -0.0302,  0.0266],\n",
              "         [ 0.0052, -0.0047, -0.0289,  ...,  0.0075,  0.0053, -0.0345]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.0172,  0.0068, -0.0045, -0.0299, -0.0233, -0.0015, -0.0045, -0.0080,\n",
              "         -0.0347, -0.0191], requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYYonG3RQ5Nb",
        "outputId": "10f22125-430d-4207-c881-2c69e02a6e51"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    outputs = model(images)\n",
        "    break\n",
        "\n",
        "print('outputs.shape : ', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:5].data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "outputs.shape :  torch.Size([128, 10])\n",
            "Sample outputs :\n",
            " tensor([[ 0.0296,  0.2195,  0.2520,  0.4365, -0.1027, -0.0537,  0.0031,  0.0519,\n",
            "         -0.1447,  0.2438],\n",
            "        [-0.0141, -0.4189, -0.2236, -0.1855, -0.2497,  0.1151,  0.4496,  0.2857,\n",
            "         -0.3106,  0.1856],\n",
            "        [ 0.1714,  0.3870,  0.2080,  0.1459, -0.1831, -0.2031, -0.0700, -0.0031,\n",
            "         -0.1093, -0.0517],\n",
            "        [ 0.0144, -0.0437,  0.0302,  0.0041, -0.2208, -0.0715,  0.0880,  0.3751,\n",
            "         -0.2200, -0.1879],\n",
            "        [ 0.3019,  0.5956,  0.3850,  0.0759,  0.1392,  0.0192,  0.1328,  0.2057,\n",
            "         -0.1969, -0.3266]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mXxodjJR3E2"
      },
      "source": [
        "For each of the 128 input images, we get 10 outputs, one for each class. But we woulod like to represent outputs as probabilities, which is not the case now, as seen above.\n",
        "\n",
        "To convert output rows to probabilities, we use the [softmax function](https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0JoWq4GSnWk"
      },
      "source": [
        "While it's easy to implement the softmax function, we'll use the implementation that's provided within PyTorch because it works well with multidimensional tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUeMNAWURyLB"
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cLiF2LPSwk7"
      },
      "source": [
        "The softmax function is included in the `torch.nn.functional` package and requires us to specify a dimension along which the function should be applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmUZU8ZqSvEm",
        "outputId": "a812a621-5a41-4429-993c-ea3d22df36cd"
      },
      "source": [
        "# Apply softmax for each output row\n",
        "probs = F.softmax(outputs,dim=1)\n",
        "\n",
        "print(\"Sample Probabilities:\\n\", probs[0:3].data)\n",
        "print(\"Sum: \", torch.sum(probs[0]).item())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample Probabilities:\n",
            " tensor([[0.0923, 0.1116, 0.1153, 0.1387, 0.0809, 0.0849, 0.0899, 0.0944, 0.0776,\n",
            "         0.1144],\n",
            "        [0.0985, 0.0657, 0.0799, 0.0830, 0.0778, 0.1121, 0.1566, 0.1329, 0.0732,\n",
            "         0.1203],\n",
            "        [0.1133, 0.1406, 0.1176, 0.1105, 0.0795, 0.0779, 0.0890, 0.0952, 0.0856,\n",
            "         0.0907]])\n",
            "Sum:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUw6GP5yTOLW"
      },
      "source": [
        "We can determine the predicted label for each image by simply choosing the index of the element with the highest probability in each output row. We can do this using `torch.max`, which returns each rows' largest element and the corresponding index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcxkMtyLTIVB",
        "outputId": "0ca46c6e-fa76-406d-d20c-9e4eeb612ee0"
      },
      "source": [
        "max_probs, preds = torch.max(probs, dim=1)\n",
        "print(preds)\n",
        "print(max_probs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3, 6, 1, 7, 1, 0, 1, 1, 0, 1, 2, 6, 7, 1, 1, 1, 7, 2, 6, 7, 6, 7, 6, 6,\n",
            "        7, 0, 7, 2, 2, 7, 3, 6, 2, 1, 6, 7, 0, 6, 1, 1, 6, 1, 3, 9, 1, 1, 1, 1,\n",
            "        7, 2, 6, 0, 2, 2, 7, 6, 0, 2, 1, 0, 1, 1, 3, 1, 1, 1, 1, 7, 3, 0, 1, 1,\n",
            "        0, 6, 7, 3, 0, 7, 6, 1, 2, 7, 6, 6, 1, 2, 1, 7, 1, 2, 6, 1, 2, 1, 1, 1,\n",
            "        6, 6, 1, 3, 1, 6, 5, 7, 1, 6, 1, 1, 7, 6, 1, 6, 2, 6, 6, 6, 1, 0, 6, 0,\n",
            "        6, 2, 3, 6, 6, 1, 6, 6])\n",
            "tensor([0.1387, 0.1566, 0.1406, 0.1467, 0.1538, 0.1269, 0.1386, 0.1481, 0.1201,\n",
            "        0.1400, 0.1484, 0.1502, 0.1222, 0.1422, 0.1333, 0.1350, 0.1266, 0.1298,\n",
            "        0.1351, 0.1243, 0.1394, 0.1355, 0.1478, 0.1356, 0.1530, 0.1406, 0.1374,\n",
            "        0.1320, 0.1595, 0.1439, 0.1261, 0.1493, 0.1255, 0.1185, 0.1324, 0.1267,\n",
            "        0.1247, 0.1235, 0.1368, 0.1329, 0.1536, 0.1221, 0.1170, 0.1095, 0.1314,\n",
            "        0.1173, 0.1405, 0.1289, 0.1322, 0.1340, 0.1421, 0.1239, 0.1371, 0.1521,\n",
            "        0.1259, 0.1221, 0.1286, 0.1337, 0.1303, 0.1213, 0.1433, 0.1407, 0.1223,\n",
            "        0.1209, 0.1213, 0.1607, 0.1392, 0.1355, 0.1365, 0.1520, 0.1433, 0.1620,\n",
            "        0.1344, 0.1747, 0.1139, 0.1165, 0.1357, 0.1265, 0.1278, 0.1407, 0.1469,\n",
            "        0.1331, 0.1165, 0.1334, 0.1343, 0.1550, 0.1227, 0.1426, 0.1217, 0.1521,\n",
            "        0.1456, 0.1625, 0.1146, 0.1383, 0.1539, 0.1250, 0.1292, 0.1403, 0.1273,\n",
            "        0.1130, 0.1276, 0.1453, 0.1156, 0.1460, 0.1524, 0.1264, 0.1365, 0.1334,\n",
            "        0.1303, 0.1254, 0.1218, 0.1337, 0.1290, 0.1213, 0.1235, 0.1397, 0.1394,\n",
            "        0.1247, 0.1360, 0.1254, 0.1262, 0.1159, 0.1261, 0.1337, 0.1358, 0.1241,\n",
            "        0.1432, 0.1365], grad_fn=<MaxBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSXvqFfXTv7x",
        "outputId": "313cd560-5070-48ee-a91a-c46398fa92a2"
      },
      "source": [
        "labels"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5, 7, 6, 2, 8, 1, 3, 2, 1, 1, 2, 7, 5, 3, 1, 3, 5, 0, 7, 3, 7, 5, 6, 6,\n",
              "        0, 1, 5, 0, 2, 3, 4, 9, 1, 0, 5, 0, 5, 7, 6, 8, 8, 1, 5, 8, 6, 1, 8, 6,\n",
              "        5, 0, 7, 9, 8, 6, 0, 4, 3, 1, 1, 9, 1, 1, 5, 5, 1, 2, 5, 3, 9, 4, 1, 8,\n",
              "        2, 4, 3, 9, 2, 3, 3, 2, 6, 5, 9, 0, 8, 2, 1, 0, 3, 6, 7, 0, 0, 1, 0, 3,\n",
              "        4, 8, 1, 4, 2, 7, 7, 5, 3, 9, 1, 3, 3, 9, 8, 6, 8, 8, 9, 9, 5, 7, 4, 7,\n",
              "        6, 9, 5, 3, 7, 4, 9, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uml0S1b2Tb7N"
      },
      "source": [
        "Compare these with the actual labels. They are obviously very different because we have just randomly initialized weights and biases. We need to now train the model i.e., adjust the parameters to make better predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWdqwZAgT2Wz"
      },
      "source": [
        "## Evaluation Metric and Loss Function\n",
        "Just as with linear regression, we need a way to evaluate how well our model is performing. A natural way to do this would be to find the percentage of labels that were predicted correctly, i.e,. the **accuracy** of the predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqQp0VDQTWh6",
        "outputId": "247dbe3e-c783-417a-a304-4f47912f3c50"
      },
      "source": [
        "torch.sum(preds==labels)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk5MJiI1UH9j"
      },
      "source": [
        "This means that 21 (will change for every run) out of 128 labels are predicted correctly. So we can define accuracy as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34ORolxSUFFz",
        "outputId": "1fdef8ec-3dbf-4929-e9f7-97d30d7b3fe0"
      },
      "source": [
        "def accuracy(outputs,labels):\n",
        "  _,preds = torch.max(outputs,dim=1)\n",
        "  return torch.tensor(torch.sum(preds==labels).item()/len(preds))\n",
        "\n",
        "accuracy(outputs,labels)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1641)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW-arjcJU4Y0"
      },
      "source": [
        "The `==` operator performs an element-wise comparison of two tensors with the same shape and returns a tensor of the same shape, containing `True` for unequal elements and `False` for equal elements. Passing the result to `torch.sum` returns the number of labels that were predicted correctly. Finally, we divide by the total number of images to get the accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFc_nPRBVHry"
      },
      "source": [
        "We can't use accuracy as a loss function for optimizing our model using gradient descent for the following reasons:\n",
        "\n",
        "1. It's not a differentiable function. `torch.max` and `==` are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n",
        "\n",
        "2. It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements. \n",
        "\n",
        "For these reasons, accuracy is often used as an **evaluation metric** for classification, but **not** as a loss function. \n",
        "\n",
        "A commonly used loss function for classification problems is the **cross-entropy**, which has the following formula:\n",
        "\n",
        "![cross-entropy](https://i.imgur.com/VDRDl1D.png)\n",
        "\n",
        "While it looks complicated, it's actually quite simple:\n",
        "\n",
        "* For each output row, pick the predicted probability for the correct label. E.g., if the predicted probabilities for an image are `[0.1, 0.3, 0.2, ...]` and the correct label is `1`, we pick the corresponding element `0.3` and ignore the rest.\n",
        "\n",
        "* Then, take the **log** of the picked probability. If the probability is high, i.e., close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiply the result by -1, which results is a large postive value of the loss for poor predictions.\n",
        "\n",
        "* Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data.\n",
        "\n",
        "Cross-entropy is a continuous and differentiable function. It also provides useful feedback for incremental improvements in the model (a slightly higher probability for the correct label leads to a lower loss). These two factors make cross-entropy a better choice for the loss function.\n",
        "\n",
        "PyTorch provides an efficient and tensor-friendly implementation of cross-entropy as part of the `torch.nn.functional` package. Moreover, it also performs softmax internally, so we can directly pass in the model's outputs without converting them into probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtRj76EOUg9I"
      },
      "source": [
        "loss_fn = F.cross_entropy"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTY33oROV30l",
        "outputId": "586a933e-301f-4d9b-c511-6efe723d28e7"
      },
      "source": [
        "loss = loss_fn(outputs,labels)\n",
        "loss"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.2809, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOf4fs1ZWf9D"
      },
      "source": [
        "Way to interpret the resulting number e.g. `2.3019` is look at `e^-2.3019` which is around `0.1` as the predicted probability of the correct label, on average. **Lower the loss, better the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgCnZuYpW0w0"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "The training process is identical to linear regression, with the addition of a validation phase, to evaluate the model in each epoch. The pseudocode is as follows:\n",
        "\n",
        "```\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    for batch in train_loader:\n",
        "        # Generate predictions\n",
        "        # Calculate loss\n",
        "        # Compute gradients\n",
        "        # Update weights\n",
        "        # Reset gradients\n",
        "    \n",
        "    # Validation phase\n",
        "    for batch in val_loader:\n",
        "        # Generate predictions\n",
        "        # Calculate loss\n",
        "        # Calculate metrics (accuracy etc.)\n",
        "    # Calculate average validation loss & metrics\n",
        "```\n",
        "\n",
        "Some parts of the training loop are specific to the specific problem we're solving (e.g. loss function, metrics etc.), whereas others are generic and can be applied to any deep learning problem. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUZLCs_fJLX2"
      },
      "source": [
        "We'll include the problem-independent parts within a function called `fit`, which will be used to train the model. The problem-specific parts will be implemented by adding new methods to the `nn.Module` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgmAnnZcfK-m"
      },
      "source": [
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    history = [] # for recording epoch-wise results\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3qfljOGJbms"
      },
      "source": [
        "The `fit` function records the validation loss and metric from each epoch. It returns a history of the training, useful for debugging & visualization.\n",
        "\n",
        "Configurations like batch size, learning rate, etc. (hyperparameters), need to be picked in advance while training ML models. Choosing the right hyperparameters is critical for training a reasonably accurate model within a reasonable amount of time.\n",
        "\n",
        "Let's define the `evaluate` function, used in the validation phase of `fit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6iwNoTzJYEa"
      },
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPVTeO7sJ62N"
      },
      "source": [
        "\n",
        "Finally, let's redefine the `MnistModel` class to include additional methods `training_step`, `validation_step`, `validation_epoch_end`, and `epoch_end` used by `fit` and `evaluate`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAqFb1OPJ6CU"
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_size, num_classes)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        xb = xb.reshape(-1, 784)\n",
        "        out = self.linear(xb)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
        "    \n",
        "model = MnistModel()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pqt_qMSKkuM"
      },
      "source": [
        "Before training the model, let's see how the model performs on the validation set with the initial set of randomly initialized weights & biases.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev5Evq5tKBr5",
        "outputId": "8775f8e1-39ad-4595-97ec-0416c249199c"
      },
      "source": [
        "result0 = evaluate(model, val_loader)\n",
        "result0"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.05488142743706703, 'val_loss': 2.3700780868530273}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shsZpQ67KuPD"
      },
      "source": [
        "The initial accuracy is around 5.5% (changes every run), which one might expect from a randomly initialized model.\n",
        "\n",
        "Now, Let's train for five epochs and look at the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfe-zWmtKo2J",
        "outputId": "e4880602-d00a-4512-fa6a-1ba899d689d9"
      },
      "source": [
        "history1 = fit(5, 0.001, model, train_loader, val_loader)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 1.9839, val_acc: 0.6227\n",
            "Epoch [1], val_loss: 1.7137, val_acc: 0.7160\n",
            "Epoch [2], val_loss: 1.5117, val_acc: 0.7517\n",
            "Epoch [3], val_loss: 1.3584, val_acc: 0.7732\n",
            "Epoch [4], val_loss: 1.2398, val_acc: 0.7919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRp4kasULFX_"
      },
      "source": [
        "With just 5 epochs of training, the model has reached an accuracy of around 80% on the validation set. We can improve this by training for a few more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNVeAn4IK6I0",
        "outputId": "4871f65a-a590-425a-865e-bc8af93a8371"
      },
      "source": [
        "history2 = fit(5, 0.001, model, train_loader, val_loader)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 1.1462, val_acc: 0.8018\n",
            "Epoch [1], val_loss: 1.0708, val_acc: 0.8122\n",
            "Epoch [2], val_loss: 1.0091, val_acc: 0.8186\n",
            "Epoch [3], val_loss: 0.9575, val_acc: 0.8237\n",
            "Epoch [4], val_loss: 0.9139, val_acc: 0.8277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQr3eSA8LRko",
        "outputId": "af89e73d-27a0-44e2-fc61-774bb7c7617b"
      },
      "source": [
        "history3 = fit(5, 0.001, model, train_loader, val_loader)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 0.8765, val_acc: 0.8314\n",
            "Epoch [1], val_loss: 0.8441, val_acc: 0.8339\n",
            "Epoch [2], val_loss: 0.8157, val_acc: 0.8369\n",
            "Epoch [3], val_loss: 0.7907, val_acc: 0.8392\n",
            "Epoch [4], val_loss: 0.7684, val_acc: 0.8416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBYKhKnwLZpT",
        "outputId": "fc98a86e-da44-4fc2-c2ec-253a08151713"
      },
      "source": [
        "history4 = fit(5, 0.001, model, train_loader, val_loader)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0], val_loss: 0.7483, val_acc: 0.8433\n",
            "Epoch [1], val_loss: 0.7302, val_acc: 0.8442\n",
            "Epoch [2], val_loss: 0.7139, val_acc: 0.8459\n",
            "Epoch [3], val_loss: 0.6990, val_acc: 0.8472\n",
            "Epoch [4], val_loss: 0.6853, val_acc: 0.8492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYt-XVYRLqDU"
      },
      "source": [
        "While the accuracy does continue to increase as we train for more epochs, the improvements get smaller with every epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "izaJatb4Lg-E",
        "outputId": "e13f04e9-f08f-47a1-d47b-20022fb459cc"
      },
      "source": [
        "history = [result0] + history1 + history2 + history3 + history4\n",
        "accuracies = [result['val_acc'] for result in history]\n",
        "plt.plot(accuracies, '-x')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. No. of epochs');"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxVd53/8dcnNysQ1kDYCwW6ULtQmaJ0UbtJXahadWhdqlY7nZFpnXbUOjqdWp0ZrTOjo+3PWh2njrali9pBB6ebth2lrdBCaYFSEoQSICGBkBDgZv38/jgn9BCy3EBObpLzfj4e95GzfM85n3vuzfdzz/d7FnN3REQkuXKyHYCIiGSXEoGISMIpEYiIJJwSgYhIwikRiIgknBKBiEjCKRGIDBFm9nUzqzGzymzHAmBmt5rZz7Idh/RMiUA6ZWZPmVmtmRVkO5bBwsxmmJmb2YoO039mZrfGvO3pwE3AXHefGOe2ZOhRIpCjmNkM4HzAgcX9vO3c/txeTBaY2cJ+3uZ0YI+77+7n7coQoEQgnfk48BxwD3B1dIaZTTOzX5hZtZntMbM7IvM+Y2YbzWy/mW0ws7PD6W5msyPl7jGzr4fDbzezCjP7Ytik8Z9mNsbMfh1uozYcnhpZfqyZ/aeZ7QznPxJOf8XM3hsplxc2lczr+AbDON8TGc8Nt3e2mRWGv+L3mNk+M1tlZqW92H+3A//Y1cxwP5WZ2V4zW25mkzNZqZmNMrP/CuPcZmZfMbMcM7sYeByYbGYNZnZPF8u/x8zWhu9ppZmdEZm31cy+FH5uteH+LcwkZjM7zcweD+dVmdnfRTabH8a838zWm9n8yHJfNLMd4bxNZnZRJvtBYuDueul1xAsoA/4KeDPQDJSG01PAS8C3geFAIXBeOO9DwA7gzwADZgMnhPMcmB1Z/z3A18PhtwMtwDeBAqAIGAdcAQwDioGHgEciy/8P8AAwBsgD3hZO/wLwQKTc5cDLXbzHW4B7I+PvBjaGw38B/CrcfircDyMz2G8zwvdaHO6Li8PpPwNuDYcvBGqAs8P3+z3gmQw/l/8C/jtc/wzgNeCayH6s6GbZecBuYEH4nq4GtgIF4fytwCvANGAs8IfIZ9RlzGEsuwiapQrD8QXhvFuBNPCucJv/DDwXzjsZ2A5Mjuy7Wdn+7if1lfUA9BpYL+A8gsq/JBx/FfibcPitQDWQ28lyjwI3dLHOnhJBE1DYTUxnAbXh8CSgDRjTSbnJwP72Sht4GPhCF+ucHZYdFo7fC9wSDn8KWAmc0ct9154IcgkSaXulF00E/wHcHllmRLi/Z/Sw7lS4n+ZGpv0F8FRkP3aXCL4PfK3DtE28kUS3AtdF5r0LKO8pZuBKYE0X27wVeCIyPhc4FNn/u4GLgbxsf++T/lLTkHR0NfCYu9eE4/fxRvPQNGCbu7d0stw0oPwYt1nt7un2ETMbZmY/CJs/6oFngNFmlgq3s9fdazuuxN13EvySvcLMRgOXEVTwR3H3MmAj8F4zG0bQF3JfOPunBIltWdj8dLuZ5fXyPf0IKI02VYUmA9sicTQAe4ApPayvhODoZ1tk2rYMlmt3AnBT2Cy0z8z2EezLaLPU9g7rbp/XXcw9fe7RM5gOAoVmlhvu/88RJIvdZrYs0yYy6XtKBHKYmRUBHwbeZmaVYZv93wBnmtmZBBXF9C46dLcDs7pY9UGCZpZ2Hc9q6XgL3JsImg4WuPtI4IL2EMPtjA0r+s78BPgoQVPVs+6+o4tyAPcT/KK9HNgQVk64e7O7f9Xd5wILgfcQ9JtkzN2bgK8CXwvjbreToFIO3pDZcIKmsO7ihKBppjm6LEEHcU/LtdsO/KO7j468hrn7/ZEy0zqse2cGMW8HTswwhiO4+33ufl64bidoHpQsUCKQqPcBrQSH8GeFr1OB/yOoCP9I0B78DTMbHnaqnhsu+yPgb83szRaYbWbtlcda4CozS5nZIuBtPcRRDBwC9pnZWOAf2me4+y7gN8D/CzuV88zsgsiyjxC0Zd9A0KbenWXApcBf8sbRAGb2DjM7PTwCqSeogNt6WFdnfkrQbr4oMu1+4JNmdpYFp+b+E/C8u2/tbkXu3go8CPyjmRWH+/ZGgmanTPwQuM7MFoSfz3Aze7eZFUfKfNbMpob7/MsE/TA9xfxrYJKZfc7MCsLYFvQUjJmdbGYXhutLE3zex7KPpS9ku21Kr4HzAv4X+NdOpn+Y4BA/l+CX4iMETQM1wHcj5a4jaHduIOh4nBdOnw+sJ2iT/ylBxRLtI6josL3JwFPhel4jaAt3wr4Jgs7MnwBVQC3wiw7L/wg4AIzI4D0/SdBZPTEy7crwfRwIt/HdyLbvAu7qYl0zonFG9p0T9hFE9lM5sJegIp0aTp8evufpXax/DEHFX03wS/wWIKer/djJ8ouAVcA+goT+EFAcztsKfAnYEM7/CWH/SXcxh/PeFO7H2vB7cnM4/VbgZ53tH+AMgh8W+yPrnJzt/4Gkviz8gESGDDO7BTjJ3T+a7VgGCzPbCnza3Z/IdizS/4bCxTsih4XNGtcAH8t2LCKDhfoIZMgws88QNJn8xt2fyXY8IoOFmoZERBJORwQiIgk36PoISkpKfMaMGdkOQ0RkUHnhhRdq3H18Z/MGXSKYMWMGq1evznYYIiKDiplt62qemoZERBJOiUBEJOGUCEREEk6JQEQk4ZQIREQSTolARGQAu+vpclaW1xwxbWV5DXc9fayP/ziaEoGIJMLxVKjZWhbgjKmjWHrfmsPrWFlew9L71nDG1FEZLZ8JJQIR6VfZqlSPp0Ltq2Xdnadf281n732R6WOK2L73IJur9vNyRR2rtu7l95treGJDFb9et5Ofv1DBvc9vY+Ou/Vwyt5RP3bOaLzz8EkvvW8MdV81j4aySHredqUF3r6H58+e7LigTOT53PV3OGVNHHVGZrCyvYV1FHde9rasHzR3/su1lo5VZx/FMlv3uknnMnzGGleU13PTgS3zjijOYN200LW1Oa5uHf9tobj1yfO32fXznic284+QJ/PbV3Xzy3BnMLBlOU0sbTa1tNLW00djSdsR4+2vHvkP88U97mTKmiIrag5xcWszwglxa2pyW1jaaWoO/LW1Oc2sbLa3B3+bWYJ2NLX3z3J3rL5zNjZee3OvlzOwFd5/f6TwlApHB6Xgq5L6ojO+4ah7nzBjLM5urufHBl/jqe0/jlEkjOdTcyqGmVtLNrYeHDzWH4+HwluoGfrepmpklw9lSfYDTp45iREEuTS1BxRmthBtbOo630tYP1VZeyshP5ZCf+8arId1C7cFmJhQXMHVMEbmpHPJSRm5ODnntw6kc8nKM3JSF03LIzTHWvL6PF16v5a0njuOiUydQmJeiIDeHwrxUh+GcI8dzU6zZXsuND7zER98ynZ89//oxHREoEYgMUHFX5q1tzsGmFg40ttLQ2MLBppbgb2Mra7fX8uM/bOXs6WN4YVstl55WyrjhBaRbWkmHFfbhCry57fC0dHMr+9PNHGo+tl+4eSmjMC8VxtbK6GF5TBxZSEGkwn2jAk6Rl7JgXqRSXr21luf/tJdzZ4/jwlNKyc0xUjlGbk5QEUfHUzlBhZzKMTZV1vPd35bxztMm8tj6Sr7ynrmcM2PsEZV9fip45eTYEXG379+PLuh9ZdwXyx5L0o5SIhCJSV81k/zLh87gTZNH8YfyGm5dvp4bLz2ZOeNHcLDpyF/VByOV8cGmFrbtOcjzW/YwcVQRO/cdYvLoIszgQGNQ+R9qbs34veQYDMvPPfyrtCgvRVF+8Iu0MD9FUTit/RfsyzvqeGFbLefNLuGdb5oYlM9LUZQf/JIdlp97eFph/hvL5qVyslKp9tVRUH8uC8f/HWunRCDSjeP9Vf7Ze1/kn95/OnNKi1lZXsO3Ht3EJ8+dwaRRRexPN1N/qIX96Wb2p1uoTzdTn24Jhg81H57e2//C9kq6KC/FwaaguWLK6CJOnRS0Ww/Lz2VEQSr8m8vwglyGF6QYnp/LsIIUIwpyea1yP1//n418+M+m8dDq7dx51dksnD2wf+Eez7LZ6hfpq4r8eCkRyJDXF5X519/3Jk4cP4KV5TV8+/HNfGTBdMYXF1AfVtp1h5oP/6071Ex9Ovib7qGJJMeguDCP4sJcRoZ/iwvzGFn0xviLr9fyh7I9XDK3lA/Mm0JhfophYWU/LD/4JV0U/souyH2j2WKw/TqGoVGpDkZKBDIo9GV7+TOvVXP9/Wv44qJTmDZ2GHsONLKnoYm9B5o6DDexp6GR+nRLl+s2g+KCXEYW5TEqfI0sDIeH5TGyMJfVW2t56rVqFp85mY+/9QRGFr1R4Q/PT2FmXa4/G5W5KuPkUSKQQSHTzs+ahkYq69JU1qepqk8fHt5UuZ+Nu+rJzTGaWjv/XucYjB1ewLjh+Ywdns+4EfnhcAFrttfy1KZq3j9vCtecN/NwZT+iMJdUTt9X5Jm+566oQpbeUCKQQeOx9ZXc9NBLvOXEcfx+cw3nzynBDCrrG6mqS1Pd0Ehrh3MHc3OMCcUFlI4qpP5QM+XVB1gwcyzvOXMyJUdU+AWMKso76mwQyM6vclBlLv0na4nAzBYB/w6kgB+5+zc6zJ8O/AQYHZa52d1XdLdOJYKBLdOKrT7dzOaqBl6r2h95NVC9v/GI9RUX5jJxZCETRxVSOrKQiSMLKR0V/A2GCxg3vIBUjg26JhaR/pSVRGBmKeA14BKgAlgFXOnuGyJl7gbWuPv3zWwusMLdZ3S3XiWCga1jJfq7V6u4YdlarjxnOg5sqtzP5qr97KxLH16mKC/FnNIRnFRaTH6usXztLt4/bzK/XreLOz9ytipzkT7QXSKI85nF5wBl7r4lDGIZcDmwIVLGgZHh8ChgZ4zxSIwaGlso393Azn1pzp1VwtU//iNFeanDnbA/eGYL+bk5zBo/gnNmjmVOaTEnlxZzUmkxU8cUkRP5RX/3x9/MwlklXHb6pIwr83UVdUeUWzirhDuumse6iroel+2ssl84q6RP7+UiMpDFeUTwQWCRu386HP8YsMDdl0bKTAIeA8YAw4GL3f2FTtZ1LXAtwPTp09+8bVuXz2CW49TTr+O9B5oo291A2e4GNu/ef3h4V+QXfl7KKC7IZe/BZhbMHMsnz53BnNJiThg7jNxU1/c51C9zkfhk64ggE1cC97j7v5rZW4Gfmtmb3P2IE7Pd/W7gbgiahrIQZ2K03ynx3z50JrmpHB7bUMkDq7Yzs2Q4P3xmC3sONB0uW5SXYvaEEbzlxHHMnjDi8GvnvkPcsGwt1184m589/zoji/KYNX5Ej9vWL3OR7IgzEewApkXGp4bToq4BFgG4+7NmVgiUALtjjEu6sHt/mtf3HGTGuGF84p5Vh6cPz08xvCCXs6aNZvaEEcyaMII5E0YweVRRp/djuWHZ2sPNNG+ZNS6W2+aKSN+JMxGsAuaY2UyCBLAEuKpDmdeBi4B7zOxUoBCojjEmiXB3ynY38NiGKp7YWMXa7ftwhymjizhz6iheqqjjmvNm8JV3z+32gqio42mrF5HsiPv00XcB3yE4NfTH7v6PZnYbsNrdl4dnCv0QGEHQcfwFd3+su3XqrKGeddfW/unzZrJ6Wy2Ph5X/tj0HgaBJ6JJTS7l4bim1B5pYev+xXSAlIgOTLihLmI6nTv721Squv38t86aN5uWddew72Ex+KoeFs8dx8amlXHxqKRNHFXa67LHe8lZEBhYlggR6fH0Vn3tgDeNGFPD63uBX/+hheVx4ygQuObWU808az4iCo1sGdeaOyNCkRJAQh5paeWJjFctf2slTm3bTHN5vZ9600dx82Sm8+YQx3Z6+KSJD10A+fVSOU3NrG78vq2H52p08tr6SA02tlI4s4JJTS/l9WQ1XL5zBvc+/Tqu7koCIdEqJYBBqa3NeeL2W5Wt38j8v72LvgSZGFuay+KzJLD5zCq1tzvXL1nDXx4IrdN+qUzhFpBtKBANUx7Z6d+eBVdv55ZodVNQeYse+QxTm5XDxqaVcftYULjiphILc1OFldQqniGRKfQQDVPvZOl9dfBrb9hxg2arXqahNk2PwtpPGc/lZU7hkbinDO+nwFRHpSH0Eg9BZ00ZzwZwS/vr+NUBwz/1PnTuDpRfOYezw/CxHJyJDiRLBAOPuLH9pJ/+84lUq69OcXDqCTVUN/NXbZ3HjpSdnOzwRGYJ0GskA8sqOOj78g2e5YdlaxhcXcOviuVQ3NB2+edvK8ppshygiQ5COCAaAPQ2N/Mtjm1i2ajtjh+XzjQ+cztQxw7h+2RrdvE1EYqdEkEXNrW389NltfPuJ1zjU1Mqnzp3J9RfNYVRRns78EZF+o7OGsuT3m2v46q/Ws3l3A+fPKeEf3juX2ROKsx2WiAxROmsoSzq7b88ja3Zw5+/K2Ly7geljh3H3x97MJXNLM77Ns4hIX1MiiFH7077uuGoeZ00bzd/94mUeWbuT/FQOn3/nyVxz3kwK81LZDlNEEk6JIEbt7frX/tcLuDsHmlo5d9Y4/vXDZx2+7bOISLYpEcTMMBoaWwD44Jun8C8fOivLEYmIHCnW6wjMbJGZbTKzMjO7uZP53zazteHrNTPbF2c8/e1QUyufe2ANOQZ/+bZZ/PbVal0LICIDTmxHBGaWAu4ELgEqgFVmttzdN7SXcfe/iZT/a2BeXPFkw00PrqWqvpGvvPtUPn3+iZx/UomuBRCRASfOI4JzgDJ33+LuTcAy4PJuyl8J3B9jPP3qxddrWfFKJRedMoFPn38icOS1ACIiA0WcfQRTgO2R8QpgQWcFzewEYCbw2y7mXwtcCzB9+vS+jTIGjS2tfOHhdUweVch3lhzZJ7BwVomOBkRkQBko9xpaAjzs7q2dzXT3u919vrvPHz9+fD+H1nvfe7KMst0N/NMHTqe4MC/b4YiIdCvORLADmBYZnxpO68wShkiz0Cs76vj+0+VccfZU3n7yhGyHIyLSozgTwSpgjpnNNLN8gsp+ecdCZnYKMAZ4NsZY+kVzaxtfeHgdY4fn8/fvOTXb4YiIZCS2RODuLcBS4FFgI/Cgu683s9vMbHGk6BJgmQ+2mx514gdPl7NhVz1ff9+bGD1MD48RkcEh1gvK3H0FsKLDtFs6jN8aZwz95bWq/Xz3yTLefcYk3nnaxGyHIyKSsYHSWTyotbY5n394HcMLUnx18WnZDkdEpFd0i4k+8OPf/4mXtu/j35ecRcmIgmyHIyLSKzoiOE5/qjnAvzy2iYtPncDiMydnOxwRkV5TIjgObW3OF3++jvzcHL7+vtP1TAERGZSUCI7Dvc9v449/2svfv3uubistIoOWEsExqqg9yDd+8yrnzynhQ/OnZjscEZFjpkRwDNydL/3iZRz4p/erSUhEBjclgmPw0AsV/N/mGm6+7BSmjR2W7XBERI6LEkEvVdWn+dqvN3DOjLF8dMEJ2Q5HROS4KRH0grvz5V++QlNLG9/84Bnk5KhJSEQGPyWCXvjVul08sbGKmy49iZklw7MdjohIn1Ai6MFdT5ezsryGPQ2N3Lp8PWdOG83cSaO46+nybIcmItIndIuJHpwxdRRL71vDyROL2Z9u5iMLpnP9suC5wyIiQ4GOCHqwcFYJf3vpSTxbvoezp4/hG795VQ+fF5EhRYkgA+2Pm3z+T3v56ILpSgIiMqQoEWTguS17ALj2ghP52fOvs7K8JssRiYj0nVgTgZktMrNNZlZmZjd3UebDZrbBzNab2X1xxnMsVpbX8PMXKshP5fCly07hjqvmsfS+NUoGIjJkxJYIzCwF3AlcBswFrjSzuR3KzAG+BJzr7qcBn4srnmO1rqKOM6eNZuqYIsyMhbNKuOOqeayrqMt2aCIifSLOI4JzgDJ33+LuTcAy4PIOZT4D3OnutQDuvjvGeI7JdW+bRUubH3F30YWzSrjubbOyGJWISN+JMxFMAbZHxivCaVEnASeZ2R/M7DkzW9TZiszsWjNbbWarq6urYwq3a5V1aSaO1G2mRWRoynZncS4wB3g7cCXwQzMb3bGQu9/t7vPdff748eP7NcC2NqeqPq3nDYjIkBVnItgBTIuMTw2nRVUAy9292d3/BLxGkBgGjJoDjUc1DYmIDCVxJoJVwBwzm2lm+cASYHmHMo8QHA1gZiUETUVbYoyp1yrr0gBqGhKRISu2RODuLcBS4FFgI/Cgu683s9vMbHFY7FFgj5ltAH4HfN7d98QV07E4nAh0RCAiQ1Ss9xpy9xXAig7TbokMO3Bj+BqQKuuVCERkaMt2Z/GAV1mXJjfHKBlekO1QRERioUTQg8q6NKUjC/UQGhEZspQIerCrTqeOisjQpkTQg6p6XUwmIkObEkE33F1HBCIy5CkRdKM+3cKh5lYmKRGIyBCmRNCN9msIStU0JCJDmBJBN9qvIdARgYgMZUoE3aisOwToiEBEhjYlgm5U1jUCSgQiMrQpEXSjsv4QJSPyyc/VbhKRoavHGs7M3mtmiawJdeqoiCRBJhX8nwObzex2Mzsl7oAGkuDJZEXZDkNEJFY9JgJ3/ygwDygH7jGzZ8NHRxbHHl2WVdanmThKN5sTkaEtoyYfd68HHiZ4AP0k4P3Ai2b21zHGllXp5lb2HWxm0igdEYjI0JZJH8FiM/sl8BSQB5zj7pcBZwI3xRte9uhiMhFJikyOCK4Avu3up7v7t9x9N4C7HwSu6W5BM1tkZpvMrMzMbu5k/ifMrNrM1oavTx/Tu4iBLiYTkaTI5AlltwK72kfMrAgodfet7v5kVwuZWQq4E7iE4CH1q8xsubtv6FD0AXdf2uvIY6YjAhFJikyOCB4C2iLjreG0npwDlLn7FndvIuhfuLz3IWbHLj2rWEQSIpNEkBtW5ACEw/kZLDcF2B4ZrwindXSFma0zs4fNbFpnKwrPUlptZqurq6sz2PTxq6pPU1yYy4iCWB/rLCKSdZkkgmozW9w+YmaXAzV9tP1fATPc/QzgceAnnRVy97vdfb67zx8/fnwfbbp7u+oO6YE0IpIImfzcvQ6418zuAIzgV/7HM1huBxD9hT81nHaYu++JjP4IuD2D9faLyvpGNQuJSCL0mAjcvRx4i5mNCMcbMlz3KmCOmc0kSABLgKuiBcxskru3d0QvBjZmGnjcKusOcdKE/jn6EBHJpowawM3s3cBpQKGZAeDut3W3jLu3mNlS4FEgBfzY3deb2W3AandfDlwfNju1AHuBTxzrG+lLLa1tVO9v1KmjIpIIPSYCM7sLGAa8g6D55oPAHzNZubuvAFZ0mHZLZPhLwJd6EW+/qG5opM2hVIlARBIgk87ihe7+caDW3b8KvBU4Kd6wsqv91FEdEYhIEmSSCNLh34NmNhloJrjf0JBV1X4Nge48KiIJkEkfwa/MbDTwLeBFwIEfxhpVluliMhFJkm4TQfhAmifdfR/wczP7NVDo7nX9El2WVNWnyc/NYcywvGyHIiISu26bhty9jeB+Qe3jjUM9CUD4ZLKRhbSfISUiMpRl0kfwpJldYQmqFYMH0qhZSESSIZNE8BcEN5lrNLN6M9tvZvUxx5VVlXVpnTEkIomRyZXFQ/6RlFHuHhwR6D5DIpIQmVxQdkFn0939mb4PJ/tqDzbT1NKmpiERSYxMTh/9fGS4kOA5Ay8AF8YSUZbtqjsEoCMCEUmMTJqG3hsdD58Z8J3YIsqyqnpdQyAiyZJJZ3FHFcCpfR3IQKGLyUQkaTLpI/gewdXEECSOswiuMB6SqurS5BiMH1GQ7VBERPpFJn0EqyPDLcD97v6HmOLJul11aSYUF5KbOpaDJRGRwSeTRPAwkHb3VgAzS5nZMHc/GG9o2VFZn9btp0UkUTK6shiI3oazCHginnCyr7IuzSSdMSQiCZJJIiiMPp4yHB6WycrNbJGZbTKzMjO7uZtyV5iZm9n8TNYbp8o63V5CRJIlk0RwwMzObh8xszcDh3payMxSBDesuwyYC1xpZnM7KVcM3AA8n2nQcWlobGF/Y4sSgYgkSiZ9BJ8DHjKznYABE4E/z2C5c4Ayd98CYGbLgMuBDR3KfQ34JkdeuJYVlYcfSKNEICLJkckFZavM7BTg5HDSJndvzmDdU4DtkfEKYEG0QHikMc3d/8fMukwEZnYtcC3A9OnTM9j0sdHFZCKSRD02DZnZZ4Hh7v6Ku78CjDCzvzreDYcPvfk34Kaeyrr73e4+393njx8//ng33SU9q1hEkiiTPoLPhE8oA8Dda4HPZLDcDmBaZHxqOK1dMfAm4Ckz2wq8BViezQ7jyvA+Q6VqGhKRBMkkEaSiD6UJO4HzM1huFTDHzGaaWT6wBFjePtPd69y9xN1nuPsM4Dlgsbuv7nx18ausTzNmWB6FealshSAi0u8ySQT/CzxgZheZ2UXA/cBvelrI3VuApcCjwEbgQXdfb2a3mdni4wk6LpV1aR0NiEjiZHLW0BcJOmqvC8fXEZw51CN3XwGs6DDtli7Kvj2Tdcapsl5PJhOR5OnxiCB8gP3zwFaCU0IvJPiFP+ToYjIRSaIujwjM7CTgyvBVAzwA4O7v6J/Q+ldTSxs1DU1MHFnUc2ERkSGku6ahV4H/A97j7mUAZvY3/RJVFrRfQ6CmIRFJmu6ahj4A7AJ+Z2Y/DDuKrZvyg1plmAh051ERSZouE4G7P+LuS4BTgN8R3Gpigpl938wu7a8A+0ulLiYTkYTKpLP4gLvfFz67eCqwhuBMoiGlPRHo9FERSZpePYbL3WvD2z1cFFdA2VJZn2ZYfoqRhZmcUSsiMnToeYyh9lNHIxdRi4gkghJBqLI+rdtPi0giKRGEdDGZiCSVEgHQ1uZU6YhARBJKiQCoOdBIS5vr1FERSSQlAnTqqIgkmxIB0YvJdJ8hEUkeJQLeuL2EOotFJImUCAiOCPJSxrjhmTx4TURkaIk1EZjZIjPbZGZlZnZzJ/OvM7OXzWytmf3ezObGGU9XKuvSTCguJCdHF5OJSPLElgjCZxvfCVwGzAWu7KSiv8/dT3f3s4DbgX+LK57u7NI1BCKSYHEeEZwDlLn7FndvApYBl0cLuHt9ZHQ44DHG06WqeiUCEUmuOBPBFGB7ZLwinHYEM/usmZUTHBFc39mKzOxaM1ttZqurq6v7NEh3D44IdOqoiP4Kf2sAAAyjSURBVCRU1juL3f1Od59FcGvrr3RR5m53n+/u88ePH9+n269Pt3CouVUXk4lIYsWZCHYA0yLjU8NpXVkGvC/GeDrVfg2BmoZEJKniTASrgDlmNtPM8oElwPJoATObExl9N7A5xng6dfgaAjUNiUhCxfYUFndvMbOlwKNACvixu683s9uA1e6+HFhqZhcDzUAtcHVc8XSlsu4QoCMCEUmuWB/H5e4rgBUdpt0SGb4hzu1nYlfYNDShWIlARJIp653F2VZVn6ZkRAH5uYnfFSKSUImv/YKLyQqyHYaISNYkPhFU1qWZOFJ3HRWR5FIiqE/rGgIRSbREJ4J0cyv7DjbrjCERSbREJ4LDF5PpGgIRSbBEJ4JduqpYRCTZiaBKTyYTEUl2ItilpiERkWQngqr6NMWFuQwviPUCaxGRAS3RiWBX3SGdOioiiZfoRFBZ30ipmoVEJOGSnQh0RCAiktxE0NLaRvX+RnUUi0jiJTYRVDc00uYwcZTuMyQiyZbYRNB+6qiahkQk6WJNBGa2yMw2mVmZmd3cyfwbzWyDma0zsyfN7IQ444mqChOBOotFJOliSwRmlgLuBC4D5gJXmtncDsXWAPPd/QzgYeD2uOLpSEcEIiKBOI8IzgHK3H2LuzcBy4DLowXc/XfufjAcfQ6YGmM8R6iqT5Ofm8PoYXn9tUkRkQEpzkQwBdgeGa8Ip3XlGuA3nc0ws2vNbLWZra6uru6T4HbVBc8hMLM+WZ+IyGA1IDqLzeyjwHzgW53Nd/e73X2+u88fP358n2yzsi6t/gEREeJNBDuAaZHxqeG0I5jZxcCXgcXu3hhjPEfQk8lERAJxJoJVwBwzm2lm+cASYHm0gJnNA35AkAR2xxjLEdydyvq0bj8tIkKMicDdW4ClwKPARuBBd19vZreZ2eKw2LeAEcBDZrbWzJZ3sbo+VXuwmaaWNl1VLCICxHr/ZXdfAazoMO2WyPDFcW6/K7vqDgE6dVREBAZIZ3F/a38ymTqLRUQSmgjeuJhM9xkSEUlkIqiqS5NjUDIiP9uhiIhkXSITwa66NBOKC8lNJfLti4gcIZE1oU4dFRF5QzITQV1ap46KiISSmwh0RCAiAiQwETQ0trC/sUWJQEQklLhEUKnnEIiIHCFxiaD9YjL1EYiIBBKXCNovJlPTkIhIIHGJoDK8z5BuLyEiEkheIqhPM2ZYHoV5qWyHIiIyICQvEdSlmah7DImIHJa8RFCfZuLIgmyHISIyYCQvEeiIQETkCLEmAjNbZGabzKzMzG7uZP4FZvaimbWY2QfjjAWgqaWNmoYmXUMgIhIRWyIwsxRwJ3AZMBe40szmdij2OvAJ4L644ojSNQQiIkeL81GV5wBl7r4FwMyWAZcDG9oLuPvWcF5bjHEcVlmvawhERDqKs2loCrA9Ml4RTus1M7vWzFab2erq6upjDqhSF5OJiBxlUHQWu/vd7j7f3eePHz/+mNejRCAicrQ4E8EOYFpkfGo4LWsq69MMy09RXBBni5iIyOASZyJYBcwxs5lmlg8sAZbHuL0etT+HwMyyGYaIyIASWyJw9xZgKfAosBF40N3Xm9ltZrYYwMz+zMwqgA8BPzCz9XHFA8ERgU4dFRE5UqxtJO6+AljRYdotkeFVBE1G/aKyLs2CE8f21+ZERAaFQdFZ3Bfa2pwqHRGIiBwlMYmg5kAjLW2ui8lERDoY8ongrqfLWVleEzl1tIiV5TXc9XR5liMTERkYhnwiOGPqKJbet4anNgUXou2uT7P0vjWcMXVUliMTERkYhnwiWDirhDuumnf4COBbj23ijqvmsXBWSZYjExEZGIZ8IoAgGbzj5AkAfGzBCUoCIiIRiUgEK8treHbLHq6/cDb3/vF1VpbXZDskEZEBY8gngpXlNSy9bw13XDWPGy89mTuumsfS+9YoGYiIhIZ8IlhXUXdEn0B7n8G6irosRyYiMjCYu2c7hl6ZP3++r169OtthiIgMKmb2grvP72zekD8iEBGR7ikRiIgknBKBiEjCKRGIiCScEoGISMINurOGzKwa2HaMi5cAA/ECAsXVO4qr9wZqbIqrd44nrhPcvdOHvg+6RHA8zGx1V6dPZZPi6h3F1XsDNTbF1TtxxaWmIRGRhFMiEBFJuKQlgruzHUAXFFfvKK7eG6ixKa7eiSWuRPURiIjI0ZJ2RCAiIh0oEYiIJNyQTARmtsjMNplZmZnd3Mn8AjN7IJz/vJnN6IeYppnZ78xsg5mtN7MbOinzdjOrM7O14euWuOMKt7vVzF4Ot3nUrV0t8N1wf60zs7P7IaaTI/thrZnVm9nnOpTpt/1lZj82s91m9kpk2lgze9zMNod/x3Sx7NVhmc1mdnXMMX3LzF4NP6dfmtnoLpbt9jOPKbZbzWxH5PN6VxfLdvv/G0NcD0Ri2mpma7tYNpZ91lXd0K/fL3cfUi8gBZQDJwL5wEvA3A5l/gq4KxxeAjzQD3FNAs4Oh4uB1zqJ6+3Ar7Owz7YCJd3MfxfwG8CAtwDPZ+EzrSS4ICYr+wu4ADgbeCUy7Xbg5nD4ZuCbnSw3FtgS/h0TDo+JMaZLgdxw+JudxZTJZx5TbLcCf5vBZ93t/29fx9Vh/r8Ct/TnPuuqbujP79dQPCI4Byhz9y3u3gQsAy7vUOZy4Cfh8MPARWZmcQbl7rvc/cVweD+wEZgS5zb70OXAf3ngOWC0mU3qx+1fBJS7+7FeUX7c3P0ZYG+HydHv0U+A93Wy6DuBx919r7vXAo8Di+KKyd0fc/eWcPQ5YGpfbKu3uthfmcjk/zeWuMI64MPA/X21vQxj6qpu6Lfv11BMBFOA7ZHxCo6ucA+XCf9p6oBx/RIdEDZFzQOe72T2W83sJTP7jZmd1k8hOfCYmb1gZtd2Mj+TfRqnJXT9z5mN/dWu1N13hcOVQGknZbK57z5FcCTXmZ4+87gsDZutftxFU0c299f5QJW7b+5ifuz7rEPd0G/fr6GYCAY0MxsB/Bz4nLvXd5j9IkHzx5nA94BH+ims89z9bOAy4LNmdkE/bbdHZpYPLAYe6mR2tvbXUTw4Th8w52Kb2ZeBFuDeLopk4zP/PjALOAvYRdAMM5BcSfdHA7Hus+7qhri/X0MxEewApkXGp4bTOi1jZrnAKGBP3IGZWR7BB32vu/+i43x3r3f3hnB4BZBnZiVxx+XuO8K/u4FfEhyeR2WyT+NyGfCiu1d1nJGt/RVR1d5EFv7d3UmZft93ZvYJ4D3AR8IK5CgZfOZ9zt2r3L3V3duAH3axzax818J64APAA12ViXOfdVE39Nv3aygmglXAHDObGf6aXAIs71BmOdDeu/5B4Ldd/cP0lbD98T+Aje7+b12UmdjeV2Fm5xB8PrEmKDMbbmbF7cMEnY2vdCi2HPi4Bd4C1EUOWePW5a+0bOyvDqLfo6uB/+6kzKPApWY2JmwKuTScFgszWwR8AVjs7ge7KJPJZx5HbNF+pfd3sc1M/n/jcDHwqrtXdDYzzn3WTd3Qf9+vvu4BHwgvgrNcXiM4++DL4bTbCP45AAoJmhrKgD8CJ/ZDTOcRHNqtA9aGr3cB1wHXhWWWAusJzpR4DljYD3GdGG7vpXDb7fsrGpcBd4b782Vgfj99jsMJKvZRkWlZ2V8EyWgX0EzQDnsNQb/Sk8Bm4AlgbFh2PvCjyLKfCr9rZcAnY46pjKDNuP071n523GRgRXefeT/sr5+G3591BJXcpI6xheNH/f/GGVc4/Z7271WkbL/ss27qhn77fukWEyIiCTcUm4ZERKQXlAhERBJOiUBEJOGUCEREEk6JQEQk4ZQIRDows1Y78s6nfXYHTDObEb3zpchAkJvtAEQGoEPufla2gxDpLzoiEMlQeD/628N70v/RzGaH02eY2W/Dm6k9aWbTw+mlFjwT4KXwtTBcVcrMfhjee/4xMyvK2psSQYlApDNFHZqG/jwyr87dTwfuAL4TTvse8BN3P4PgJm/fDad/F3jag5vinU1wRSrAHOBOdz8N2AdcEfP7EemWriwW6cDMGtx9RCfTtwIXuvuW8CZhle4+zsxqCG6X0BxO3+XuJWZWDUx198bIOmYQ3D9+Tjj+RSDP3b8e/zsT6ZyOCER6x7sY7o3GyHAr6quTLFMiEOmdP4/8fTYcXklwl0yAjwD/Fw4/CfwlgJmlzGxUfwUp0hv6JSJytCI78gHm/+vu7aeQjjGzdQS/6q8Mp/018J9m9nmgGvhkOP0G4G4zu4bgl/9fEtz5UmRAUR+BSIbCPoL57l6T7VhE+pKahkREEk5HBCIiCacjAhGRhFMiEBFJOCUCEZGEUyIQEUk4JQIRkYT7/1mZVdVDKPLoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMjHyrl5L2Rv"
      },
      "source": [
        "It's quite clear from the above plot that the model probably won't cross the 90% even after training for a long time. \n",
        "\n",
        "One possible reason for this is that the learning rate might be too high. The model's parameters may be bouncing around the optimal set of parameters for the lowest loss. You can try reducing the learning rate and training for a few more epochs to see if it helps.\n",
        "\n",
        "The more likely reason that **the model just isn't powerful enough**. Our hypothesis was that, we assumed the output (in this case the class probabilities) is a **linear function** of the input (pixel intensities), obtained by perfoming a matrix multiplication with the weights matrix and adding the bias. This is a fairly weak assumption, as there may not be a linear relationship between the pixel intensities in an image and the digit it represents. While it works reasonably well for a simple dataset like MNIST (getting us to 85% accuracy), we need more sophisticated models that can capture non-linear relationships between image pixels and labels for complex tasks like recognizing everyday objects, animals etc. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H45P7v9AMckg"
      },
      "source": [
        "## Testing with individual images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cdMc8XhLyLt"
      },
      "source": [
        "# Define test dataset\n",
        "test_dataset = MNIST(root='data/', \n",
        "                     train=False,\n",
        "                     transform=transforms.ToTensor())"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WntEyCB1MjTy"
      },
      "source": [
        "Here is a sample image from the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "hhcqIZugMhur",
        "outputId": "34b9e2ac-5bb6-42e9-b139-2bc6c7acf2ae"
      },
      "source": [
        "img, label = test_dataset[0]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Shape:', img.shape)\n",
        "print('Label:', label)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: torch.Size([1, 28, 28])\n",
            "Label: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmTW6mgvMpxC"
      },
      "source": [
        "Let's define a helper function `predict_image`, which returns the predicted label for a single image tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeXl23zUMniZ"
      },
      "source": [
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    _, preds = torch.max(yb, dim=1)\n",
        "    return preds[0].item()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wj1SJksMu2w"
      },
      "source": [
        "`img.unsqueeze` simply adds another dimension at the begining of the 1x28x28 tensor, making it a 1x1x28x28 tensor, which the model views as a batch containing a single image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "g5A6PvYfM3Qv",
        "outputId": "75391a8d-cce0-4e22-ab3b-c122eac65e10"
      },
      "source": [
        "img, label = test_dataset[0]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 7 , Predicted: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "MAVFHHe8M5EJ",
        "outputId": "4a702342-11f7-4548-b019-ef1e676bb242"
      },
      "source": [
        "img, label = test_dataset[10]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 0 , Predicted: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "E-1p4AkxM6_a",
        "outputId": "402d3e93-437e-4f49-d639-8e2de28ac0d3"
      },
      "source": [
        "img, label = test_dataset[193]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 9 , Predicted: 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "LhpZimGFM868",
        "outputId": "60fc8870-bbd4-43e5-c8c3-6ebbbd8f69ac"
      },
      "source": [
        "img, label = test_dataset[1839]\n",
        "plt.imshow(img[0], cmap='gray')\n",
        "print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label: 2 , Predicted: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6Rd0K6NDxE"
      },
      "source": [
        "Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing/decreasing the complexity of the model, and changing the hypeparameters.\n",
        "\n",
        "As a final step, let's look at the overall loss and accuracy of the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foXcbxbGM_gi",
        "outputId": "1a8b7305-d293-418f-dc24-44769f08c15c"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "result = evaluate(model, test_loader)\n",
        "result"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.8573242425918579, 'val_loss': 0.6515902280807495}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2DInBuXNOVr"
      },
      "source": [
        "## Saving and loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9vwchlVNIo6"
      },
      "source": [
        "torch.save(model.state_dict(), 'mnist-logistic.pth')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzm9bFTcNUkt"
      },
      "source": [
        "The `.state_dict` method returns an `OrderedDict` containing all the weights and bias matrices mapped to the right attributes of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeUMzoG8NS4T",
        "outputId": "b97cc3a2-bacb-4c42-a27c-1a663fd1aeb2"
      },
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[-0.0243,  0.0129,  0.0147,  ...,  0.0313,  0.0038, -0.0035],\n",
              "                      [ 0.0309,  0.0061,  0.0232,  ...,  0.0260,  0.0330,  0.0064],\n",
              "                      [-0.0323, -0.0092, -0.0100,  ...,  0.0127,  0.0013,  0.0154],\n",
              "                      ...,\n",
              "                      [-0.0079,  0.0045, -0.0171,  ..., -0.0139,  0.0288,  0.0334],\n",
              "                      [ 0.0019,  0.0287,  0.0244,  ...,  0.0080, -0.0075, -0.0024],\n",
              "                      [ 0.0321,  0.0170,  0.0285,  ...,  0.0087, -0.0005,  0.0015]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0252,  0.1082, -0.0500, -0.0130,  0.0080,  0.0234, -0.0098,  0.0449,\n",
              "                      -0.0571, -0.0383]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKLoX6uPNioK"
      },
      "source": [
        "To load the model weights, we can instante a new object of the class `MnistModel`, and use the `.load_state_dict` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_kjMWGYNYFT"
      },
      "source": [
        "model2 = MnistModel()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIcCwt7VNlFy",
        "outputId": "7715a882-e11b-4afc-f8b2-e2985b804367"
      },
      "source": [
        "model2.state_dict() # Randomly initialized model"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[-0.0142,  0.0039,  0.0135,  ...,  0.0125, -0.0066,  0.0290],\n",
              "                      [ 0.0179, -0.0124, -0.0322,  ..., -0.0191,  0.0191,  0.0331],\n",
              "                      [-0.0171,  0.0241,  0.0208,  ..., -0.0165,  0.0087,  0.0250],\n",
              "                      ...,\n",
              "                      [-0.0085, -0.0182,  0.0294,  ..., -0.0011,  0.0229, -0.0132],\n",
              "                      [-0.0004,  0.0148, -0.0304,  ...,  0.0127,  0.0189,  0.0068],\n",
              "                      [ 0.0275,  0.0202,  0.0290,  ..., -0.0308,  0.0230, -0.0024]])),\n",
              "             ('linear.bias',\n",
              "              tensor([ 0.0137,  0.0033,  0.0023,  0.0317,  0.0340, -0.0027, -0.0167, -0.0328,\n",
              "                      -0.0268, -0.0341]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1g4g2HDOfHk",
        "outputId": "76461c3a-501c-4731-930f-3f7e5eae8189"
      },
      "source": [
        "evaluate(model2, test_loader)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.12812499701976776, 'val_loss': 2.3194963932037354}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ifK5DEmOaQV"
      },
      "source": [
        "Again, since this is a randomly initialized model, we expect such performance.\n",
        "\n",
        "Now, Let's load our trained model into `model2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMH4UzdHOTK_",
        "outputId": "7a3a4487-1691-4dfb-e292-19805bdd8054"
      },
      "source": [
        "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
        "model2.state_dict()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[-0.0243,  0.0129,  0.0147,  ...,  0.0313,  0.0038, -0.0035],\n",
              "                      [ 0.0309,  0.0061,  0.0232,  ...,  0.0260,  0.0330,  0.0064],\n",
              "                      [-0.0323, -0.0092, -0.0100,  ...,  0.0127,  0.0013,  0.0154],\n",
              "                      ...,\n",
              "                      [-0.0079,  0.0045, -0.0171,  ..., -0.0139,  0.0288,  0.0334],\n",
              "                      [ 0.0019,  0.0287,  0.0244,  ...,  0.0080, -0.0075, -0.0024],\n",
              "                      [ 0.0321,  0.0170,  0.0285,  ...,  0.0087, -0.0005,  0.0015]])),\n",
              "             ('linear.bias',\n",
              "              tensor([-0.0252,  0.1082, -0.0500, -0.0130,  0.0080,  0.0234, -0.0098,  0.0449,\n",
              "                      -0.0571, -0.0383]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kBzoKgrOwSg"
      },
      "source": [
        "You can see that the trained paramteres have been loaded into our new `model2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC2rmzI2OuH7",
        "outputId": "53f5dc92-acdf-4b6f-b798-afecae05ecad"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=256)\n",
        "result = evaluate(model2, test_loader)\n",
        "result"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'val_acc': 0.8573242425918579, 'val_loss': 0.6515902280807495}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_NzgbuhPkeX"
      },
      "source": [
        "## Summary and Further Reading\n",
        "\n",
        "We covered:\n",
        "\n",
        "* Working with images in PyTorch (using the MNIST dataset).\n",
        "* Splitting a dataset into training, validation and test sets.\n",
        "* Creating PyTorch models with custom logic by extending the `nn.Module` class.\n",
        "* Interpreting model ouputs as probabilities using softmax, and picking predicted labels.\n",
        "* Picking a good evaluation metric (accuracy) and loss function (cross entropy) for classification problems.\n",
        "* Setting up a training loop that also evaluates the model using the validation set.\n",
        "* Testing the model manually on randomly picked examples \n",
        "* Saving and loading model checkpoints to avoid retraining from scratch.\n",
        "\n",
        "**Experiment!**\n",
        "\n",
        "* Try making the validation set smaller or larger, and see how it affects the model.\n",
        "* Try changing the learning rate and see if you can achieve the same accuracy in fewer epochs.\n",
        "* Try changing the batch size. What happens if you use too high a batch size, or too low?\n",
        "* Modify the `fit` function to also track the overall loss and accuracy on the training set, and see how it compares with the validation loss/accuracy. Can you explain why it's lower/higher?\n",
        "* Train with a small subset of the data, and see if you can reach a similar level of accuracy.\n",
        "* Try building a model for a different dataset, such as the [CIFAR10 or CIFAR100 datasets](https://www.cs.toronto.edu/~kriz/cifar.html).\n"
      ]
    }
  ]
}